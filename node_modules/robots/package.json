{
  "name": "robots",
  "version": "0.9.4",
  "description": "Parser for robots.txt",
  "keywords": [
    "robots.txt",
    "robotstxt",
    "parser"
  ],
  "homepage": "https://github.com/ekalinin/robots.js",
  "repository": {
    "type": "git",
    "url": "git://github.com/ekalinin/robots.js.git"
  },
  "author": {
    "name": "Eugene Kalinin",
    "email": "e.v.kalinin@gmail.com"
  },
  "engines": {
    "node": ">= 0.6.0"
  },
  "devDependencies": {
    "expresso": "0.8.1"
  },
  "main": "index",
  "bugs": {
    "url": "https://github.com/ekalinin/robots.js/issues"
  },
  "licenses": [
    {
      "type": "MIT"
    }
  ],
  "readme": "robots.js\n=========\n\nrobots.js — is parser for [robots.txt](www.robotstxt.org) files for node.js.\n\nInstallation\n------------\n\nIt's recommended to install via [npm](https://github.com/isaacs/npm/):\n\n```bash\n$ npm install -g robots\n```\n\nUsage\n-----\n\nHere's an example of using robots.js:\n\n```javascript\nvar robots = require('robots')\n  , parser = new robots.RobotsParser();\n\nparser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {\n  if(success) {\n    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access) {\n      if (access) {\n        // parse url\n      }\n    });\n  }\n});\n```\n\nDefault crawler user-agent is:\n\n    Mozilla/5.0 (X11; Linux i686; rv:5.0) Gecko/20100101 Firefox/5.0\n\nHere's an example of using another user-agent and more detailed callback:\n\n```javascript\nvar robots = require('robots')\n  , parser = new robots.RobotsParser(\n                'http://nodeguide.ru/robots.txt',\n                'Mozilla/5.0 (compatible; RobotTxtBot/1.0)',\n                after_parse\n            );\n            \nfunction after_parse(parser, success) {\n  if(success) {\n    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access, url, reason) {\n      if (access) {\n        console.log(' url: '+url+', access: '+access);\n        // parse url ...\n      }\n    });\n  }\n};\n```\n\nHere's an example of getting list of sitemaps:\n\n```javascript\nvar robots = require('robots')\n  , parser = new robots.RobotsParser();\n\nparser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {\n  if(success) {\n    parser.getSitemaps(function(sitemaps) {\n      // sitemaps — array\n    });\n  }\n});\n```\n\nHere's an example of getCrawlDelay usage:\n\n```javascript\n    var robots = require('robots')\n      , parser = new robots.RobotsParser();\n\n    // for example:\n    //\n    // $ curl -s http://nodeguide.ru/robots.txt\n    //\n    // User-agent: Google-bot\n    // Disallow: / \n    // Crawl-delay: 2\n    //\n    // User-agent: *\n    // Disallow: /\n    // Crawl-delay: 2\n\n    parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {\n      if(success) {\n        var GoogleBotDelay = parser.getCrawlDelay(\"Google-bot\");\n        // ...\n      }\n    });\n```\n\nAn example of passing options to the HTTP request:\n\n```javascript\nvar options = {\n  headers:{\n    Authorization:\"Basic \" + new Buffer(\"username:password\").toString(\"base64\")}\n}\n\nvar robots = require('robots')\n  , parser = new robots.RobotsParser(null, options);\n\nparser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {\n  ...\n});\n```\n\n\nAPI\n---\n\nRobotsParser — main class. This class provides a set of methods to read,\nparse and answer questions about a single robots.txt file.\n\n  * **setUrl(url, read)** — sets the URL referring to a robots.txt file.\n    by default, invokes read() method.\n    If read is a function, it is called once the remote file is downloaded and parsed, and it\n      takes in two arguments: the first is the parser itself, and the second is a boolean\n      which is True if the the remote file was successfully parsed.\n  * **read(after_parse)** — reads the robots.txt URL and feeds it to the parser\n  * **parse(lines)** — parse the input lines from a robots.txt file\n  * **canFetch(userAgent, url, callback)** — using the parsed robots.txt decide if\n    userAgent can fetch url. Callback function:\n    ``function callback(access, url, reason) { ... }``\n    where:\n    * *access* — can this url be fetched. true/false.\n    * *url* — target url\n    * *reason* — reason for ``access``. Object:\n      * type — valid values: 'statusCode', 'entry', 'defaultEntry', 'noRule'\n      * entry — an instance of ``lib/Entry.js:``. Only for types: 'entry', 'defaultEntry'\n      * statusCode — http response status code for url. Only for type 'statusCode'\n  * **canFetchSync(userAgent, url)** — using the parsed robots.txt decide if\n    userAgent can fetch url. Return true/false.\n  * **getCrawlDelay(userAgent)** — returns Crawl-delay for the certain userAgent\n  * **getSitemaps(sitemaps)** — gets Sitemaps from parsed robots.txt\n\nLicense\n-------\n\nSee [LICENSE](https://github.com/ekalinin/robots.js/blob/master/LICENSE)\nfile.\n\n\nResources\n=========\n\n  * [Robots.txt Specifications by Google](http://code.google.com/web/controlcrawlindex/docs/robots_txt.html)\n  * [Robots.txt parser for python](http://docs.python.org/library/robotparser.html)\n  * [A Standard for Robot Exclusion](http://www.robotstxt.org/orig.html)\n",
  "readmeFilename": "README.md",
  "_id": "robots@0.9.4",
  "_from": "robots@"
}
